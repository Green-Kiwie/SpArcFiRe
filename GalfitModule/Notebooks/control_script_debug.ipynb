{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b74c8abe-15bb-4a67-ba56-ca8eeb720eac",
   "metadata": {},
   "source": [
    "**Author: Matthew Portman**\n",
    "\n",
    "**Date (Github date will likely be more accurate): 4/17/23**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ef73de-bec0-40d9-8389-b5016b67ffa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import join as pj\n",
    "\n",
    "from astropy.io import fits\n",
    "\n",
    "import argparse\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92295f7f-b5c3-4bc1-9708-d7f0faa2777c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For debugging purposes\n",
    "from IPython import get_ipython\n",
    "def in_notebook():\n",
    "    ip = get_ipython()\n",
    "    \n",
    "    if ip:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18265963-f373-4dc1-93f6-a3139b9eaa9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Could not find GalfitModule!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/sparcfire_matt/GalfitModule/Utilities/combine_via_parallel.py:19\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     _SPARCFIRE_DIR \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSPARCFIRE_HOME\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     20\u001b[0m     _MODULE_DIR \u001b[38;5;241m=\u001b[39m pj(_SPARCFIRE_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGalfitModule\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<frozen os>:679\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SPARCFIRE_HOME'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mFunctions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelper_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mUtilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel_residual_calc\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mparallel_residual_calc\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mUtilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombine_via_parallel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcombine_via_parallel\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# This should give me numpy and pandas and whatnot\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# also gives this, from os.path import join as pj\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msparc_to_galfit_feedme_gen\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/sparcfire_matt/GalfitModule/Utilities/combine_via_parallel.py:30\u001b[0m\n\u001b[1;32m     27\u001b[0m     _MODULE_DIR \u001b[38;5;241m=\u001b[39m pj(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGalfitModule\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists(_MODULE_DIR):\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find GalfitModule!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(_MODULE_DIR)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mClasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mComponents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mException\u001b[0m: Could not find GalfitModule!"
     ]
    }
   ],
   "source": [
    "_HOME_DIR = os.path.expanduser(\"~\")\n",
    "if in_notebook():\n",
    "    # We hardcode the directory name because the notebooks \n",
    "    # are used for prototyping and debugging. \n",
    "    _SPARCFIRE_DIR = pj(_HOME_DIR, \"sparcfire_matt\") \n",
    "    _MODULE_DIR    = pj(_SPARCFIRE_DIR, \"GalfitModule\")\n",
    "else:\n",
    "    try:\n",
    "        _SPARCFIRE_DIR = os.environ[\"SPARCFIRE_HOME\"]\n",
    "        _MODULE_DIR = pj(_SPARCFIRE_DIR, \"GalfitModule\")\n",
    "    except KeyError:\n",
    "        \n",
    "        if __name__ == \"__main__\":\n",
    "            print(\"SPARCFIRE_HOME is not set. Please run 'setup.bash' inside SpArcFiRe directory if not done so already.\")\n",
    "            print(\"Checking the current directory for GalfitModule, otherwise quitting.\")\n",
    "            \n",
    "        _MODULE_DIR = pj(os.getcwd(), \"GalfitModule\")\n",
    "        \n",
    "        if not exists(_MODULE_DIR):\n",
    "            raise Exception(\"Could not find GalfitModule!\")\n",
    "    \n",
    "sys.path.append(_MODULE_DIR)\n",
    "\n",
    "from Classes.Components import *\n",
    "from Classes.Containers import *\n",
    "from Functions.helper_functions import *\n",
    "import Utilities.parallel_residual_calc as parallel_residual_calc\n",
    "import Utilities.combine_via_parallel as combine_via_parallel\n",
    "\n",
    "# This should give me numpy and pandas and whatnot\n",
    "# also gives this, from os.path import join as pj\n",
    "from sparc_to_galfit_feedme_gen import *\n",
    "import go_go_galfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32377d52-fa4f-467a-b40b-3d56e5b5cec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # TODO: Add run_dir? and default to cwd if not specified\n",
    "    \n",
    "    # Force >python 3.7 for various compatabilities\n",
    "    out_str = \"\\t Python3.7 or greater required! Exitting without generating feedmes...\"\n",
    "    assert sys.version_info >= (3, 7), out_str\n",
    "    \n",
    "    cwd = absp(os.getcwd()) # Doesn't work *in* notebook\n",
    "    old_cwd = absp(cwd) # Strings are immutable\n",
    "    \n",
    "    username = os.environ[\"USER\"]\n",
    "    \n",
    "    USAGE = f\"\"\"USAGE:\n",
    "\n",
    "    python3 ./{sys.argv[0]} [OPTION] [[RUN-DIRECTORY] IN-DIRECTORY TMP-DIRECTORY OUT-DIRECTORY]\n",
    "    \n",
    "    OPTIONS =>[-p | --parallel]\n",
    "              [-drs | --dont-remove-slurm]\n",
    "              [-t  | --tmp]\n",
    "              [-ac | --aggressive-clean]\n",
    "              [-NS | --num-steps] \n",
    "              [-r | --restart]\n",
    "              [-nsf | --no-simultaneous-fitting]\n",
    "              [-v | --verbose]\n",
    "              [-n | --name]\n",
    "\n",
    "    This script is the wrapping script for running GALFIT using SpArcFiRe to inform \n",
    "    the input. By default, it runs from the RUN (or current) directory and uses the\n",
    "    '-in' '-tmp' and '-out' directories as specified or otherwise defaults to \n",
    "    'sparcfire-in', 'sparcfire-tmp', 'sparcfire-out'. \n",
    "\n",
    "    Please do not specify symlinks for the above, they discomfort the programmer.\n",
    "    \"\"\"\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description = USAGE)\n",
    "    \n",
    "    parser.add_argument('-p', '--parallel',\n",
    "                        dest     = 'parallel',\n",
    "                        action   = 'store',\n",
    "                        type     = int,\n",
    "                        choices  = range(0,3),\n",
    "                        default  = 1,\n",
    "                        help     = 'Run algorithm with/without intensive parallelization. Defaults to on machine parallel.\\nOptions are:\\n\\t\\\n",
    "                                    0: in serial,\\n\\t\\\n",
    "                                    1: on machine parallel,\\n\\t\\\n",
    "                                    2: cluster computing via SLURM'\n",
    "                       )\n",
    "\n",
    "    parser.add_argument('-drs', '--dont-remove-slurm',\n",
    "                        dest     = 'dont_remove_slurm',\n",
    "                        action   = 'store_const',\n",
    "                        const    = True,\n",
    "                        default  = False,\n",
    "                        help     = 'Choose NOT to remove all old slurm files (they may contain basic info about each fit but there will be a bunch!)'\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument('-t', '--tmp',\n",
    "                        dest     = 'run_from_tmp',\n",
    "                        action   = 'store_const',\n",
    "                        const    = True,\n",
    "                        default  = False,\n",
    "                        help     = 'Indicates to the program a run from a directory in /tmp/ local to each cluster machine.\\n\\\n",
    "                                    WARNING: either output to a different location or copy from tmp to said location \\\n",
    "                                    under the assumption that tmp will be wiped at some point in the near future.'\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument('-ac', '--aggressive-clean',\n",
    "                        dest     = 'aggressive_clean',\n",
    "                        action   = 'store_const',\n",
    "                        const    = True,\n",
    "                        default  = False,\n",
    "                        help     = 'Aggressively clean-up directories, removing -in, temp output, psf, and mask files after galfit runs'\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument('-NS', '--num-steps',\n",
    "                        dest     = 'steps', \n",
    "                        action   = 'store',\n",
    "                        type     = int,\n",
    "                        choices  = range(1,4),\n",
    "                        default  = 2,\n",
    "                        help     = 'Run GALFIT using step-by-step component selection (up to 3), i.e.\\n\\t\\\n",
    "                                    1: Bulge + Disk + Arms,\\n\\t\\\n",
    "                                    2: Bulge -> Bulge + Disk + Arms,\\n\\t\\\n",
    "                                    3: Bulge -> Bulge + Disk -> Bulge + Disk + Arms'\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument('-r', '--restart',\n",
    "                        dest     = 'restart',\n",
    "                        action   = 'store_const',\n",
    "                        const    = True,\n",
    "                        default  = False,\n",
    "                        help     = 'Restart control script on the premise that some have already run (likely in parallel).'\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument('-nsf', '--no-simultaneous-fitting',\n",
    "                        dest     = 'simultaneous_fitting',\n",
    "                        action   = 'store_const',\n",
    "                        const    = False,\n",
    "                        default  = True,\n",
    "                        help     = 'Turn off simultaneous fitting.'\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument('-n', '--basename',\n",
    "                        dest     = 'basename', \n",
    "                        action   = 'store',\n",
    "                        type     = str,\n",
    "                        default  = \"GALFIT\",\n",
    "                        help     = 'Basename of the output results pkl file ([name]_output_results.pkl).'\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument('-v', '--verbose',\n",
    "                        dest     = 'verbose', \n",
    "                        action   = 'store_const',\n",
    "                        const    = True,\n",
    "                        default  = False,\n",
    "                        help     = 'Verbose output for all bash commands in control script.'\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument(dest     = 'paths',\n",
    "                        nargs    = \"*\",\n",
    "                        type     = str,\n",
    "                        help     = \"RUN-DIRECTORY [IN-DIRECTORY TMP-DIRECTORY OUT-DIRECTORY] from SpArcFiRe. \\\n",
    "                                    SpArcFiRe directories should follow -in, -tmp, -out.\"\n",
    "                       )\n",
    "    \n",
    "    if not in_notebook():\n",
    "        args              = parser.parse_args() # Using vars(args) will call produce the args as a dict\n",
    "        num_steps         = args.steps\n",
    "        parallel          = args.parallel\n",
    "        dont_remove_slurm = args.dont_remove_slurm\n",
    "        run_from_tmp      = args.run_from_tmp\n",
    "        aggressive_clean  = args.aggressive_clean\n",
    "        \n",
    "        restart           = args.restart\n",
    "        basename          = args.basename\n",
    "        \n",
    "        # TODO: This isn't working, forcing false for now\n",
    "        simultaneous_fitting = False #args.simultaneous_fitting\n",
    "        \n",
    "        verbose           = args.verbose\n",
    "        capture_output    = not args.verbose\n",
    "        \n",
    "        # if num_steps not in range(1,4):\n",
    "        #     print(\"The number of steps you selected cannot be used!\")\n",
    "        #     print(\"Using two.\")\n",
    "        #     print()\n",
    "        #     num_steps = 2\n",
    "\n",
    "        if len(args.paths) == 1:\n",
    "            cwd = args.paths[0]\n",
    "            in_dir = pj(cwd, \"sparcfire-in\")\n",
    "            tmp_dir = pj(cwd, \"sparcfire-tmp\")\n",
    "            out_dir = pj(cwd, \"sparcfire-out\")\n",
    "            \n",
    "        elif len(args.paths) == 3:\n",
    "            in_dir, tmp_dir, out_dir = args.paths[0], args.paths[1], args.paths[2]\n",
    "            #print(f\"Paths are, {in_dir}, {tmp_dir}, {out_dir}\")\n",
    "            \n",
    "        elif len(args.paths) == 4:\n",
    "            cwd, in_dir, tmp_dir, out_dir = args.paths[0], args.paths[1], args.paths[2], args.paths[3]\n",
    "            #print(f\"Paths are, {in_dir}, {tmp_dir}, {out_dir}\")\n",
    "            \n",
    "        else:\n",
    "            in_dir = pj(cwd, \"sparcfire-in\")\n",
    "            tmp_dir = pj(cwd, \"sparcfire-tmp\")\n",
    "            out_dir = pj(cwd, \"sparcfire-out\")\n",
    "            print(f\"Paths incorrectly specified, defaulting to {cwd} (-in, -tmp, -out)...\")\n",
    "            print(f\"{in_dir}\\n{tmp_dir}\\n{out_dir}\")\n",
    "            print()\n",
    "            \n",
    "        check_dir_names = [1 for i in (in_dir, tmp_dir, out_dir) if \"-\" not in i ]\n",
    "        if check_dir_names:\n",
    "            raise Exception(\"Directory paths must end in '-in' '-tmp' and '-out'\")\n",
    "            \n",
    "    else:\n",
    "        parallel = 0\n",
    "        #rerun = \"\"\n",
    "        num_steps = 2\n",
    "        # Avoid some... nasty surprises for when debugging\n",
    "        restart = True\n",
    "        verbose = False\n",
    "        capture_output = True\n",
    "        \n",
    "        cwd = cwd.replace(\"ics-home\", username)\n",
    "        in_dir = pj(cwd, \"sparcfire-in\")\n",
    "        tmp_dir = pj(cwd, \"sparcfire-tmp\")\n",
    "        out_dir = pj(cwd, \"sparcfire-out\")\n",
    "        \n",
    "        sys.path.append(pj(_HOME_DIR, \".local\", \"bin\"))\n",
    "        \n",
    "    # Making these absolute paths\n",
    "    cwd     = absp(cwd)\n",
    "    in_dir  = absp(in_dir)\n",
    "    tmp_dir = absp(tmp_dir)\n",
    "    out_dir = absp(out_dir)\n",
    "    \n",
    "    # Changing to specified working dir\n",
    "    os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ba02eb-ebf9-4e0a-97ed-6d8df59162a0",
   "metadata": {},
   "source": [
    "## Checking file directories and installed programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d01744-c99d-4e70-b1ae-04a5545df77b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Checking dirs\n",
    "    \n",
    "    assert all((exists(in_dir), exists(tmp_dir), exists(out_dir))), \\\n",
    "           f\"Cannot find one of: in, tmp, or out directories:\\n{in_dir}\\n{tmp_dir}\\n{out_dir}\\n\" \\\n",
    "           \"Do those look right?\"\n",
    "    \n",
    "#     # to redirect stderr to /dev/null as well:\n",
    "#     #subprocess.run(['ls', '-l'], stderr=subprocess.DEVNULL)\n",
    "\n",
    "    if not in_notebook():\n",
    "        # This is in helper_functions\n",
    "        run_galfit, run_fitspng, run_python = check_programs()\n",
    "#         # This seems to work in Python directly so I'm leaving it as-is\n",
    "#         # Checking galfit\n",
    "#         run_galfit = shutil.which(\"galfit\")\n",
    "#         #run_galfit = response.stdout.strip()\n",
    "        \n",
    "#         # Checking fitspng\n",
    "#         run_fitspng   = shutil.which(\"fitspng\")\n",
    "#         fitspng_param = \"0.25,1\" #1,150\"\n",
    "        \n",
    "#         # Checking exact python3 call\n",
    "#         run_python = shutil.which(\"python3\")\n",
    "\n",
    "    else:\n",
    "        run_galfit  = pj(_HOME_DIR, \".local/bin/galfit\")\n",
    "        run_fitspng = pj(_HOME_DIR, \".local/bin/fitspng\")\n",
    "        run_python  = \"/opt/conda/bin/python3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46afa543-182b-4f70-abc1-7512ab09e291",
   "metadata": {},
   "source": [
    "## Setting up directories and handy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf5af5-a0af-4fd6-b86d-fc6805b443eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Setting up paths and variables\n",
    "    tmp_fits_dir    = pj(tmp_dir, \"galfits\")\n",
    "    tmp_masks_dir   = pj(tmp_dir, \"galfit_masks\")\n",
    "    # Now dropping these in the individual galaxy folders\n",
    "    #tmp_psf_dir     = pj(tmp_dir, \"psf_files\")\n",
    "    tmp_png_dir     = pj(tmp_dir, \"galfit_png\")\n",
    "    \n",
    "    sim_fitting_dir = pj(tmp_dir, \"sim_fitting\")\n",
    "    if simultaneous_fitting:\n",
    "        sf_in_dir       = pj(sim_fitting_dir, \"sparcfire-in\")\n",
    "        sf_tmp_dir      = pj(sim_fitting_dir, \"sparcfire-tmp\")\n",
    "        sf_out_dir      = pj(sim_fitting_dir, \"sparcfire-out\")\n",
    "        sf_masks_dir    = pj(sf_tmp_dir, \"galfit_masks\")\n",
    "        \n",
    "        # These really the necessary ones, tmp and masks not so much but kept for posterity\n",
    "        for path in [sim_fitting_dir, sf_in_dir, sf_out_dir]:\n",
    "            if not exists(path):\n",
    "                simultaenous_fitting = False\n",
    "                print(f\"Simultaneous Fitting cannot be performed, {path} does not exist!\")\n",
    "                print(\"Continuing...\")\n",
    "                break\n",
    "        \n",
    "    #need_masks_dir  = pj(tmp_dir, \"need_masks\")\n",
    "    \n",
    "    #all_galfit_out = pj(out_dir, \"all_galfit_out\")\n",
    "    out_png_dir     = pj(out_dir, \"galfit_png\")\n",
    "    \n",
    "    # Should be same as SpArcFiRe's but I'm packaging it with just in case\n",
    "    star_removal_path = pj(_MODULE_DIR, \"star_removal\")\n",
    "    \n",
    "    if parallel == 1:\n",
    "        # CPU Parallel\n",
    "        pipe_to_parallel_cmd = \"/home/sana/bin/parallel\"\n",
    "        \n",
    "    elif parallel == 2:\n",
    "        # SLURM/Cluster Computing\n",
    "        pipe_to_parallel_cmd = \"~wayne/bin/distrib_slurm\"\n",
    "    \n",
    "    if not restart:\n",
    "        # Remove old\n",
    "        print(\"Removing old results in tmp folder (if they exist).\")\n",
    "        _ = sp(f\"rm -rf {tmp_fits_dir}\", capture_output = capture_output)\n",
    "        # try:\n",
    "        #     shutil.rmtree(tmp_fits_dir)\n",
    "        # except OSError as e:\n",
    "        #     pass\n",
    "    else:\n",
    "        print(\"Restarting the run (still have to find everything).\")\n",
    "\n",
    "    # Making sub-directories\n",
    "    _ = [os.mkdir(i) for i in (tmp_fits_dir, \n",
    "                               tmp_masks_dir, \n",
    "                               #tmp_psf_dir, \n",
    "                               tmp_png_dir, \n",
    "                               out_png_dir\n",
    "                              )#, \n",
    "                               #need_masks_dir) \n",
    "         if not exists(i)\n",
    "        ]\n",
    "    \n",
    "    if simultaneous_fitting:\n",
    "        _ = [os.mkdir(i) for i in (sf_tmp_dir, \n",
    "                                   sf_masks_dir\n",
    "                                  )#, \n",
    "                               #need_masks_dir) \n",
    "         if not exists(i)\n",
    "        ]\n",
    "    \n",
    "    # makedirs will make both at once, handy!\n",
    "    #if not exists(out_png): os.makedirs(out_png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d674546-203d-45cd-b454-c36b07410c97",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running sextractor (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6270c1ef-738e-4837-a43c-02b16cfb92f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":    \n",
    "    # Grabbing list of file names and masks with bash variable expansion\n",
    "    print(\"Setting up, finding files...\")\n",
    "    # input_filenames = glob.glob(pj(in_dir, \"*.fits\"))\n",
    "    # output_folders  = glob.glob(pj(out_dir, \"123*/\"))\n",
    "    # star_masks      = glob.glob(pj(tmp_dir, \"*_star-rm.fits\"))\n",
    "    input_filenames = find_files(in_dir, \"*.fits\", \"f\")\n",
    "    if not input_filenames:\n",
    "        print(f\"No input files found in {in_dir}. Is something wrong?\")\n",
    "        sys.exit()\n",
    "        \n",
    "    output_folders  = [i for i in find_files(out_dir, \"*\", \"d\") \n",
    "                       if os.path.basename(i) != os.path.basename(out_dir) and\n",
    "                          os.path.basename(i) != \"galfit_png\"\n",
    "                      ]\n",
    "    \n",
    "    star_masks      = find_files(tmp_dir, \"*_star-rm.fits\", \"f\")\n",
    "    \n",
    "    # The ONLY reason we need this is because of how remove_stars_with... works\n",
    "    # If we change that, the whole mess of code and operations that use can go away\n",
    "    # the need_masks_dir can go away\n",
    "    #in_need_masks   = glob.glob(pj(need_masks_dir, \"*.fits\"))\n",
    "#     in_need_masks    = find_files(need_masks_dir, \"*.fits\", \"f\")\n",
    "    \n",
    "#     check_need_masks = set(output_folders).intersection(\n",
    "#                        set([os.path.basename(i) for i in in_need_masks])\n",
    "#                                                         )\n",
    "#     if not check_need_masks:\n",
    "#         _ = sp(f\"rm -rf {need_masks_dir}\", capture_output = capture_output)\n",
    "#         os.mkdir(need_masks_dir)\n",
    "#         in_need_masks = []\n",
    "    \n",
    "    # Sparcfire plops them into tmp so I just move them to where I need them\n",
    "    if star_masks and not restart:\n",
    "        sp(f\"mv {pj(tmp_dir,'*_star-rm.fits')} {tmp_masks_dir}\", capture_output = capture_output)\n",
    "        \n",
    "    #star_masks = glob.glob(pj(tmp_masks_dir, \"*_star-rm.fits\"))\n",
    "    #star_masks      = find_files(tmp_masks_dir, \"*_star-rm.fits\", \"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c44e832-54ad-4892-be51-6625bb525ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    generate_starmasks = True\n",
    "    if not restart:\n",
    "        #generate_starmasks = True\n",
    "        if not exists(pj(tmp_masks_dir, 'remove_stars_with_sextractor.log')):\n",
    "            # remove_stars_with_sextractor needs this available before it can log\n",
    "            _ = sp(f\"touch {pj(tmp_masks_dir, 'remove_stars_with_sextractor.log')}\", capture_output = capture_output)\n",
    "            \n",
    "        if simultaneous_fitting and not exists(pj(sf_masks_dir, 'remove_stars_with_sextractor.log')):\n",
    "            # remove_stars_with_sextractor needs this available before it can log\n",
    "            _ = sp(f\"touch {pj(sf_masks_dir, 'remove_stars_with_sextractor.log')}\", capture_output = capture_output)\n",
    "        \n",
    "#         try:\n",
    "#             star_removal_path = pj(os.environ[\"SPARCFIRE_HOME\"], \"star_removal\")\n",
    "            \n",
    "#         except KeyError as k:\n",
    "#             print(\"SPARCFIRE_HOME environment variable is not set.\")\n",
    "#             print(\"Will use 'remove_stars_with_sextractor.py' from GalfitModule\")\n",
    "#             print(\"If they have already been generated, ignore this message.\")\n",
    "#             print()\n",
    "            \n",
    "        # else:\n",
    "        \n",
    "        # Compare against output folders because extra observations may be in input directory\n",
    "        # GALFIT can only run on what's there... I mean there are defaults for SpArcFiRe\n",
    "        # but this is the more appropriate choice. \n",
    "        # Now done in go_go_galfit\n",
    "        # if len(output_folders) != len(star_masks):\n",
    "        #     print(\"The temp directory has a different number of star masks than the number of output directories.\") \n",
    "        # else:\n",
    "        #     generate_starmasks = False\n",
    "            \n",
    "# DEPRECATED\n",
    "                           \n",
    "#             print(\"Getting things ready to generate star masks...\")\n",
    "#             #galaxy_folder_names = [os.path.basename(i.rstrip(\"/\")) for i in output_folders]\n",
    "#             #star_mask_names = [os.path.basename(i) for i in star_masks]\n",
    "            \n",
    "#             parallel_copy_input = \"parallel_copy_inputs\"\n",
    "#             if parallel:\n",
    "#                 if exists(parallel_copy_input):\n",
    "#                     _ = sp(f\"rm {parallel_copy_input}\", capture_output = capture_output)\n",
    "\n",
    "#                 _ = sp(f\"touch {parallel_copy_input}\", capture_output = capture_output)\n",
    "\n",
    "#                 sci = open(parallel_copy_input, \"a\")\n",
    "                \n",
    "#             # TODO: Could also tar then transfer(?)\n",
    "#             if len(in_need_masks) + len(star_mask_names) != len(output_folders):\n",
    "#                 print(\"Copying input galaxies without masks to 'need_masks' in tmp folder\")\n",
    "#                 for gname in galaxy_folder_names:\n",
    "#                     star_mask_filename = f\"{gname}_star-rm.fits\"\n",
    "#                     in_file = f\"{gname}.fits\"\n",
    "#                     cp_cmd = f\"cp -uv {pj(in_dir, in_file)} {need_masks_dir}\"\n",
    "\n",
    "#                     if in_file in in_need_masks:\n",
    "#                         continue\n",
    "                    \n",
    "#                     # This is probably unecessary but keeping just in case\n",
    "#                     elif in_file.split(\".fits\")[0] not in galaxy_folder_names:\n",
    "#                         _ = sp(f\"rm -f {pj(need_masks_dir, in_file)}\", capture_output = capture_output)\n",
    "\n",
    "#                     elif star_mask_filename not in star_mask_names:\n",
    "\n",
    "#                         if parallel:\n",
    "#                             cp_cmd += \"\\n\"\n",
    "#                             sci.write(cp_cmd)\n",
    "\n",
    "#                         else:\n",
    "#                             try:\n",
    "#                                 shutil.copy2(pj(in_dir, in_file), need_masks_dir)\n",
    "#                             except FileNotFoundError:\n",
    "#                                 print(f\"Could not find {gname} in {in_dir}. Continuing...\")\n",
    "#                             #result = sp(f\"cp -u {pj(need_masks_dir, in_file)}\", capture_output = True)\n",
    "#                             # if result.stderr:\n",
    "#                             #     print(f\"Could not find {gname} in {in_dir}, {result.stderr}. Continuing...\")\n",
    "#                     else:\n",
    "#                         # To remove extras, print out the rest of this list\n",
    "#                         star_mask_names.remove(star_mask_filename)\n",
    "#                         _ = sp(f\"rm -f {pj(need_masks_dir, in_file)}\", capture_output = capture_output)\n",
    "\n",
    "#                 if parallel:\n",
    "#                     sci.close()\n",
    "\n",
    "#                     extra_parallel = \"\"\n",
    "#                     if verbose:\n",
    "#                         extra_parallel = \"-v\"\n",
    "\n",
    "#                     parallel_run_name = \"COPYING_INPUT_FILES\"\n",
    "#                     parallel_run_cmd = f\"cat {parallel_copy_input} | {pipe_to_parallel_cmd} {parallel_run_name} -M all {extra_parallel}\"\n",
    "#                     # Running without timeout for now\n",
    "#                     print(\"Performing the copy with parallel...\")\n",
    "#                     _ = sp(f\"{parallel_run_cmd}\", capture_output = capture_output)\n",
    "                \n",
    "#             print(\"Generating starmasks...\")\n",
    "#             os.chdir(star_removal_path)\n",
    "#             out_text = sp(f\"python3 {pj(star_removal_path, 'remove_stars_with_sextractor.py3')} {need_masks_dir} {tmp_masks_dir}\", capture_output = capture_output)\n",
    "#             os.chdir(cwd)\n",
    "            \n",
    "#             #star_masks = glob.glob(pj(tmp_masks_dir, \"*_star-rm.fits\"))\n",
    "#             star_masks = find_files(tmp_masks_dir, \"*_star-rm.fits\", \"f\")\n",
    "#             issue_masking_gnames = [os.path.basename(sm).rstrip(\"_star-rm.fits\") \n",
    "#                                     for sm in star_masks\n",
    "#                                     if sm not in galaxy_folder_names\n",
    "#                                    ]\n",
    "            \n",
    "#             # GALFIT can still run on these galaxies without a mask \n",
    "#             # so I don't have to worry about letting it know\n",
    "#             if issue_masking_gnames:\n",
    "#                 issue_file = pj(tmp_dir, \"issue_masking.txt\")\n",
    "                \n",
    "#                 # This is so that I don't have to generate all star masks again...\n",
    "#                 with open(issue_file, \"w\") as f:\n",
    "#                     for im in issue_masking_gnames:\n",
    "#                         _ = sp(f\"touch {pj(tmp_masks_dir, im)}-failed_star-rm.fits\")\n",
    "#                         f.write(im)\n",
    "                    \n",
    "#                 print(\"Could not produce a star mask for all galaxies\")\n",
    "#                 print(f\"See {issue_file} for more details, creating empties...\")\n",
    "            \n",
    "#             if out_text.stderr:\n",
    "#                 print(f\"Something went wrong running 'remove_stars_with_sextractor.py'! Printing debug info...\")\n",
    "#                 print(out_text)\n",
    "#                 #print(type(out_text.stderr))\n",
    "#             else:\n",
    "#                 print(\"Star masks have been generated successfully.\")\n",
    "#                 print()\n",
    "                \n",
    "    # else:\n",
    "    #     generate_starmasks = False\n",
    "    #     print(\"Star masks have already been generated, proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea68dc-0ec7-495d-8a93-1bfe6025a2ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Galfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7fcee8-b2f4-4248-9940-2f0993b768b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_parallel(cwd, \n",
    "                   kwargs_main, \n",
    "                   galfit_script_name = pj(_MODULE_DIR, \"go_go_galfit.py\"), \n",
    "                   parallel_file = pj(cwd, \"parallel_cmd_file\"),\n",
    "                   # determined by parallel proc limit \n",
    "                   chunk_size = 20\n",
    "                  ):\n",
    "    _, _, run_python = check_programs()\n",
    "    kwargs_in        = deepcopy(kwargs_main)\n",
    "    \n",
    "    print(f\"Generating input file for parallelization in {cwd}: {parallel_file}\")\n",
    "    with open(pj(cwd, parallel_file), \"w\") as scf:\n",
    "        #for gname in kwargs_main[\"galaxy_names\"]:\n",
    "        for i, chunk in enumerate(range(chunk_size, len(kwargs_main[\"galaxy_names\"]) +  chunk_size, chunk_size)):\n",
    "            chunk_o_galaxies = kwargs_main[\"galaxy_names\"][chunk - chunk_size:][:chunk_size]\n",
    "            kwargs_in[\"galaxy_names\"] = \",\".join(chunk_o_galaxies)\n",
    "            \n",
    "            # kwargs_in[\"petromags\"] = \",\".join(kwargs_main[\"petromags\"][chunk - chunk_size:][:chunk_size])\n",
    "            # kwargs_in[\"bulge_axis_ratios\"] = \",\".join(kwargs_main[\"bulge_axis_ratios\"][chunk - chunk_size:][:chunk_size])\n",
    "            \n",
    "\n",
    "            cmd_str = \"\"\n",
    "            for k,v in kwargs_in.items():\n",
    "                cmd_str += f\"{k}={v} \"\n",
    "                \n",
    "            # Good thing dictionaries retain order now, *whistles innocently*\n",
    "            scf.write(f\"{run_python} {galfit_script_name} {cmd_str}\\n\")\n",
    "\n",
    "    sp(f\"chmod a+x {parallel_file}\", capture_output = capture_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d0d7c8-9731-4c6c-98f3-d2767108098d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_galfit_out_hangups(tmp_fits_dir, out_dir, kwargs_main):\n",
    "    # For hang-ups, check if final copy has occurred\n",
    "    # Check for file which indicates galfit outputs nothing at all\n",
    "    # This avoids conflating ones which haven't run and ones which have nothing to show for it\n",
    "    kwargs_main[\"galaxy_names\"] = [gname for gname in kwargs_main[\"galaxy_names\"]\n",
    "                                   if not exists(f\"{pj(out_dir, gname, gname)}_galfit_out.fits\")\n",
    "                                   and not exists(f\"{pj(tmp_fits_dir, 'failed_' + gname)}_galfit_out.fits\")]\n",
    "    # TODO: CHECK FOR FINAL .IN FILE TO ENSURE IT TRIED TO RUN THE FULL FIT\n",
    "    \n",
    "    # because of mutability I don't need to do this but\n",
    "    # because it's good practice...\n",
    "    return kwargs_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f337ecf7-9108-4a69-8fcd-403595d37ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_failed(failed_dir = cwd, failures = []):\n",
    "    if failures:\n",
    "        fail_filename = \"galfit_failed.txt\"\n",
    "        fail_filepath = pj(failed_dir, fail_filename)\n",
    "        print(f\"{len(failures)} galax(y)ies completely failed. Writing the list of these to {fail_filepath}\")\n",
    "        with open(fail_filepath, \"w\") as ff:\n",
    "            ff.writelines(\"\\n\".join(failures))\n",
    "            ff.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0107afc-4b40-4c86-b82b-8df344be9b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":    \n",
    "    #print(\"Finding all galaxies...\")\n",
    "    # galaxy_names = [os.path.basename(i).rstrip(\".fits\") \n",
    "    #                 for i in input_filenames]\n",
    "    \n",
    "    # Replace is safer than rstrip because rstrip could remove those characters\n",
    "    # from the end of the filename\n",
    "    galaxy_names = [i.replace(\".fits\", \"\") for i in input_filenames]\n",
    "    \n",
    "    if not restart:\n",
    "        # Backing up old fits\n",
    "        # So we can check for new ones\n",
    "        # and also backup previous runs if there's an accidental overwrite\n",
    "        print(\"Backing up previous output (if found)\")\n",
    "        for gname in galaxy_names:\n",
    "            new_out = pj(out_dir, gname, f\"{gname}_galfit_out.fits\")\n",
    "            old_out = pj(out_dir, gname, f\"{gname}_galfit_out_old.fits\")\n",
    "            if exists(new_out):\n",
    "                shutil.move(new_out, old_out)\n",
    "                \n",
    "    #print(\"Reading in SDSS information\")\n",
    "    # TODO: don't hardcode this\n",
    "    # ASSUME ALL ARE IN SAME COLOR BAND (FOR NOW)\n",
    "    # with fits.open(pj(in_dir, galaxy_names[0] + \".fits\")) as f:\n",
    "    #     #SURVEY = SDSS-r  DR7\n",
    "    #     color = f[0].header[\"SURVEY\"].split()[0][-1]\n",
    "        \n",
    "#     gzoo_file = pj(_HOME_DIR, \"kelly_stuff\", \"Kelly-Final-GZ-all-psfield-incl.csv\")\n",
    "#     try:\n",
    "#         gzoo_data = pd.read_csv(gzoo_file, \n",
    "#                                 #sep = \"\\t\", \n",
    "#                                 usecols = [\"GZ_dr8objid\", \"petroMag_r\", \"deVAB_r\"],\n",
    "#                                 index_col = \"GZ_dr8objid\", \n",
    "#                                 dtype = {\"GZ_dr8objid\" : str}\n",
    "#                                )\n",
    "        \n",
    "#         petromags = [str(gzoo_data.loc[gname, f\"petroMag_{color}\"]) if gname in gzoo_data.index else \"16\" \n",
    "#                      for gname in galaxy_names]\n",
    "        \n",
    "#         bulge_axis_ratios = [str(gzoo_data.loc[gname, f\"deVAB_{color}\"]) if gname in gzoo_data.index else \"1\"\n",
    "#                              for gname in galaxy_names]\n",
    "        \n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Could not find {gzoo_file}. Proceeding.\")\n",
    "#         gzoo_data = None\n",
    "\n",
    "    kwargs_main = {\"cwd\"                  : cwd,\n",
    "                   \"in_dir\"               : in_dir,\n",
    "                   \"tmp_dir\"              : tmp_dir,\n",
    "                   \"out_dir\"              : out_dir,\n",
    "                   \"num_steps\"            : num_steps,\n",
    "                   #\"rerun\"                : rerun,\n",
    "                   \"parallel\"             : parallel,\n",
    "                   \"verbose\"              : verbose,\n",
    "                   \"capture_output\"       : capture_output,\n",
    "                   \"generate_starmasks\"   : generate_starmasks,\n",
    "                   \"run_from_tmp\"         : run_from_tmp,\n",
    "                   \"aggressive_clean\"     : aggressive_clean,\n",
    "                   \"simultaneous_fitting\" : simultaneous_fitting,\n",
    "                   \"sim_fitting_dir\"      : sim_fitting_dir,\n",
    "                   # \"petromags\"          : petromags,\n",
    "                   # \"bulge_axis_ratios\"  : bulge_axis_ratios,\n",
    "                   # Keep this last just in case\n",
    "                   \"galaxy_names\"       : galaxy_names\n",
    "                  }\n",
    "    \n",
    "    # In case we're running back to back, this will reduce galaxy_names appropriately\n",
    "    kwargs_main = check_galfit_out_hangups(tmp_fits_dir, out_dir, kwargs_main)\n",
    "    parallel_file = \"parallel_cmd_file\"\n",
    "    \n",
    "#    raise(AssertionError())\n",
    "    # One at a time\n",
    "    if parallel:\n",
    "        if not kwargs_main[\"galaxy_names\"] and not restart:\n",
    "            print(\"No galaxies to fit, exitting.\")\n",
    "            sys.exit()\n",
    "            \n",
    "        #print(\"Piping to parallel\")\n",
    "        print(f\"{len(kwargs_main['galaxy_names'])} galaxies\")\n",
    "        \n",
    "        chunk_size = 5\n",
    "        if parallel == 1:\n",
    "            # For CPU parallel\n",
    "            parallel_run_name = \"\"#\"GALFITTING\"\n",
    "            parallel_options  = joblib.cpu_count()\n",
    "            parallel_verbose  = \"\"\n",
    "            chunk_size = len(kwargs_main[\"galaxy_names\"])//joblib.cpu_count() + 1\n",
    "            # Two whole days for big runs\n",
    "            timeout = 2880 # Minutes\n",
    "            \n",
    "        elif parallel == 2:\n",
    "            # For SLURM/Cluster Computing\n",
    "            parallel_run_name = \"GALFITTING\"\n",
    "            # Slurm needs different timeout limits\n",
    "            timeout = 60 # Minutes\n",
    "            parallel_options  = f\"-M all -t {timeout}\"\n",
    "            parallel_verbose  = \"-v\" if verbose else \"\"\n",
    "            chunk_size = 20\n",
    "\n",
    "            \n",
    "        parallel_run_cmd = f\"cat {parallel_file} | {pipe_to_parallel_cmd} {parallel_run_name} {parallel_options} {parallel_verbose}\"\n",
    "        \n",
    "        if not restart:\n",
    "            write_to_parallel(cwd, kwargs_main, parallel_file = parallel_file, chunk_size = chunk_size)\n",
    "            print(\"Galfitting via parallelization...\")\n",
    "            try:\n",
    "                sp(f\"{parallel_run_cmd}\", capture_output = capture_output, timeout = 60*(timeout + 1))\n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(\"Timed out.\")\n",
    "                pass\n",
    "\n",
    "            # Python needs a moment to catch-up it seems\n",
    "            time.sleep(60)\n",
    "            kwargs_main = check_galfit_out_hangups(tmp_fits_dir, out_dir, kwargs_main)\n",
    "        else:\n",
    "            print(\"Restart fitting commencing\")\n",
    "        \n",
    "        count = 2\n",
    "        while kwargs_main[\"galaxy_names\"] and count < 10:\n",
    "            print(\"Did not finish all galaxies, parallelizing again...\\n\")\n",
    "            print(f\"{len(kwargs_main['galaxy_names'])} galaxies to go.\")\n",
    "            write_to_parallel(cwd, kwargs_main, parallel_file = parallel_file, chunk_size = chunk_size)\n",
    "            \n",
    "            try:\n",
    "                print(\"Piping to parallel\")\n",
    "                sp(f\"{parallel_run_cmd}\", capture_output = capture_output, timeout = 60*(timeout + 1))\n",
    "            except subprocess.TimeoutExpired:\n",
    "                pass\n",
    "            \n",
    "            time.sleep(60)\n",
    "            kwargs_main = check_galfit_out_hangups(tmp_fits_dir, out_dir, kwargs_main)\n",
    "            count += 1\n",
    "            \n",
    "        if count == 10 or kwargs_main[\"galaxy_names\"]:\n",
    "            print(\"Did not finish all galaxies. There is likely an error. Check parallel output files if possible. Restart with -r option.\")\n",
    "            print(\"Quitting.\")\n",
    "            sys.exit()\n",
    "            \n",
    "#             run_galfit.main(cwd          = cwd,\n",
    "#                             in_dir       = in_dir,\n",
    "#                             tmp_dir      = tmp_dir,\n",
    "#                             out_dir      = out_dir,\n",
    "#                             num_steps    = num_steps,\n",
    "#                             rerun        = rerun,\n",
    "#                             galaxy_names = gname\n",
    "#                            )\n",
    "        \n",
    "    else:\n",
    "        failures = go_go_galfit.main(**kwargs_main, \n",
    "                                     run_galfit = run_galfit, \n",
    "                                     run_fitspng = run_fitspng, \n",
    "                                     run_python = run_python)\n",
    "        \n",
    "        write_failed(out_dir, failures)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45e03dbf-75b3-4ef2-bbea-60f4a7668834",
   "metadata": {
    "tags": []
   },
   "source": [
    "    # Unused but here for a good time\n",
    "    boom = \"\"\"Numerical Recipes run-time error...\n",
    "gaussj: Singular Matrix-1\n",
    "...now exiting to system...\n",
    "\n",
    "                 __/~*##$%@@@******~\\-__\n",
    "               /f=r/~_-~ _-_ --_.^-~--\\=b\\\n",
    "             4fF / */  .o  ._-__.__/~-. \\*R\\\n",
    "            /fF./  . /- /' /|/|  \\_  * *\\ *\\R\\\n",
    "           (iC.I+ '| - *-/00  |-  \\  )  ) )|RB\n",
    "           (I| (  [  / -|/^^\\ |   )  /_/ | *)B\n",
    "           (I(. \\ `` \\   \\m_m_|~__/ )_ .-~ F/\n",
    "            \\b\\\\=_.\\_b`-+-~x-_/ .. ,._/ , F/\n",
    "             ~\\_\\= =  =-*###%#x==-#  *=- =/\n",
    "                ~\\**U/~  | i i | ~~~\\===~\n",
    "                        | I I \\\\\n",
    "                       / // i\\ \\\\\n",
    "                  (   [ (( I@) )))  )\n",
    "                       \\_\\_VYVU_/\n",
    "                         || * |\n",
    "                        /* /I\\ *~~\\\n",
    "                      /~-/*  / \\ \\ ~~M~\\\n",
    "            ____----=~ // /WVW\\* \\|\\ ***===--___\n",
    "\n",
    "   Doh!  GALFIT crashed because at least one of the model parameters\n",
    "   is bad.  The most common causes are: effective radius too small/big,\n",
    "   component is too far outside of fitting region (also check fitting\n",
    "   region), model mag too faint, axis ratio too small, Sersic index\n",
    "   too small/big, Nuker powerlaw too small/big.  If frustrated or\n",
    "   problem should persist, email for help or report problem to:\n",
    "                     Chien.Y.Peng@gmail.com\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9184c0-472c-4cbe-aa18-e8c32bcf31fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tidying Up in case anything is leftover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e22c03-3612-4661-8b37-6d29797695fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Cleaning up...\")\n",
    "    _ = sp(\"rm galfit.* fit.log\", capture_output = capture_output)\n",
    "    _ = sp(\"rm *.png\", capture_output = capture_output)\n",
    "    \n",
    "    # We use the negative of remove slurm because we want cleanup to be the default\n",
    "    if parallel and not dont_remove_slurm:\n",
    "        _ = sp(f\"rm -r \\\"$HOME/SLURM_turds/{parallel_run_name}\\\"\", capture_output = capture_output)\n",
    "        _ = sp(f\"rm {parallel_file}\", capture_output = capture_output)\n",
    "        #_ = sp(f\"rm {parallel_copy_input}\", capture_output = capture_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd4653b-85f1-4195-a45e-e2f9e25e797f",
   "metadata": {},
   "source": [
    "## Combining Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f62eec2-8cb3-4345-8898-bce9821b9d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #basename = \"GALFIT\"\n",
    "    pkl_end_str = \"output_results\"\n",
    "    final_pkl_file = pj(out_dir, f\"{basename}_{pkl_end_str}.pkl\")\n",
    "    print(f\"Combining all the residual calculations into {final_pkl_file}\")\n",
    "\n",
    "    #all_nmr = {}\n",
    "    output_df = pd.DataFrame()\n",
    "    \n",
    "    if parallel:\n",
    "        python_parallel   = pj(_MODULE_DIR, \"Utilities\", \"combine_via_parallel.py\")\n",
    "        parallel_file     = \"parallel_combine_residual\"\n",
    "        if exists(parallel_file):\n",
    "            _ = sp(f\"rm -f {parallel_file}\", capture_output = capture_output)\n",
    "        \n",
    "        if parallel == 1:\n",
    "            # For CPU parallel\n",
    "            parallel_run_name = \"\"#\"GALFITTING\"\n",
    "            parallel_options  = joblib.cpu_count() #\"-M all\"\n",
    "            \n",
    "        elif parallel == 2:\n",
    "            # For SLURM/Cluster Computing\n",
    "            parallel_run_name = \"COMBINE_RESULTS\"\n",
    "            parallel_options  = \"-M all\"\n",
    "\n",
    "        finished_pkl_num = 0\n",
    "        if restart:\n",
    "            check_output_pkl = [int(os.path.basename(i).split(\"_\")[0].replace(basename, \"\")) \n",
    "                                for i in find_files(tmp_dir, f'{basename}*_{pkl_end_str}.pkl', \"f\")\n",
    "                                if os.path.basename(i).split(\"_\")[0].replace(basename, \"\")\n",
    "                                ]\n",
    "            if check_output_pkl:\n",
    "                finished_pkl_num = max(check_output_pkl)\n",
    "\n",
    "        chunk_size = 20\n",
    "        # Use count for restarting purposes i.e. if all pkl files have been generated \n",
    "        # but just need to be combined\n",
    "        count = 0\n",
    "        with open(parallel_file, \"w\") as sf:\n",
    "            for i, chunk in enumerate(range(chunk_size, len(galaxy_names) + chunk_size, chunk_size)):\n",
    "                if i < finished_pkl_num:\n",
    "                    continue\n",
    "\n",
    "                gal_to_parallel = galaxy_names[chunk - chunk_size:][:chunk_size]\n",
    "                #num_str = f\"{i:0>3}\"\n",
    "                sf.write(f\"{run_python} {python_parallel} {pj(tmp_dir, basename + str(i))} {pkl_end_str} {out_dir} {','.join(gal_to_parallel)}\\n\")\n",
    "                count += 1\n",
    "\n",
    "        if count:\n",
    "            print(\"parallelizing to combine residuals\")\n",
    "            parallel_run_cmd = f\"cat {parallel_file} | {pipe_to_parallel_cmd} {parallel_run_name} {parallel_options} {parallel_verbose}\"\n",
    "            _ = sp(parallel_run_cmd, capture_output = capture_output)\n",
    "\n",
    "        all_output_pkl = [pj(tmp_dir, fname) \n",
    "                          for fname in find_files(tmp_dir, f'{basename}*_{pkl_end_str}.pkl', \"f\")\n",
    "                          if fname != f\"{basename}_{pkl_end_str}.pkl\"\n",
    "                         ]\n",
    "        #_ = [all_nmr.update(pickle.load(open(file, 'rb'))) for file in all_output_pkl]\n",
    "        out_df = pd.concat(\n",
    "                           [pd.read_pickle(file) for file in all_output_pkl \n",
    "                            if os.path.basename(file) != f\"{basename}_{pkl_end_str}.pkl\"\n",
    "                           ]\n",
    "                          ) \n",
    "        \n",
    "    else:\n",
    "        # for gname in galaxy_names:\n",
    "        #     output_file = pj(out_dir, gname, f\"{gname}_galfit_out.fits\")\n",
    "        #     if exists(output_file):\n",
    "        #         with fits.open(output_file) as hdul: \n",
    "        #             all_nmr[gname] = (hdul[2].header.get(\"NMR\", None), \n",
    "        #                               hdul[2].header.get(\"ks_p\", None),\n",
    "        #                               hdul[2].header.get(\"ks_stat\", None)\n",
    "        #                              )\n",
    "        #basename         = args[0]\n",
    "        #out_dir          = args[1] #os.path.dirname(basename)\n",
    "        #galaxy_names     = args[2].split(\",\")\n",
    "        \n",
    "        # In this case it's not parallel but I'm just saving some hassle here\n",
    "        out_df = combine_via_parallel.main(\"\", pkl_end_str, out_dir, \",\".join(galaxy_names))\n",
    "\n",
    "    # Could split this into the above if/else but this keeps everything output\n",
    "    # related in one place\n",
    "    print(f\"Outputting results to {final_pkl_file}\")\n",
    "    \n",
    "    # For when I do it in one of the other scripts\n",
    "    if \"gname\" in out_df.columns:\n",
    "        out_df.set_index(\"gname\", inplace = True)\n",
    "        \n",
    "    #pickle.dump(all_nmr, open(final_pkl_file, 'wb'))\n",
    "    out_df.to_pickle(final_pkl_file)\n",
    "    \n",
    "    if not dont_remove_slurm and parallel:\n",
    "        _ = sp(f\"rm -r \\\"$HOME/SLURM_turds/{parallel_run_name}\\\"\", capture_output = capture_output)\n",
    "        _ = sp(f\"rm -f {pj(tmp_dir, basename)}*_{pkl_end_str}.pkl\", capture_output = capture_output)\n",
    "        _ = sp(f\"rm -f {parallel_file}\", capture_output = capture_output)\n",
    "        \n",
    "    #_ = sp(f\"mv {pickle_filename_temp} {pkl_file}\", capture_output = capture_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e3ea2-b9f7-45a2-aff9-a9cc4846fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprecated\n",
    "# Combine the residuals now that they're in the headers\n",
    "    # print(\"Calculating residuals\")\n",
    "    # parallel_residual_calc.main(run_dir           = cwd,\n",
    "    #                             # Don't actually need in_dir\n",
    "    #                             #in_dir            = in_dir,\n",
    "    #                             tmp_dir           = tmp_dir,\n",
    "    #                             out_dir           = out_dir,\n",
    "    #                             basename          = \"GALFIT\",\n",
    "    #                             parallel             = parallel,\n",
    "    #                             dont_remove_parallel = dont_remove_parallel,\n",
    "    #                             restart           = restart,\n",
    "    #                             verbose           = verbose,\n",
    "    #                             capture_output    = not verbose\n",
    "    #                            )\n",
    "    # if aggressive_clean:\n",
    "    #     # May need to use find and delete\n",
    "    #     print(\"Aggressively cleaning, removing star masks.\")\n",
    "    #     _ = sp(f\"rm -rf {pj(tmp_masks_dir)} {pj(tmp_png_dir)}\", capture_output = capture_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa2287-c58c-4b65-98e6-328b63d869b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if aggressive_clean:\n",
    "        print(\"Final tidying...\")\n",
    "        _ = sp(f\"rm -rf {out_dir}\", capture_output = capture_output)\n",
    "        _ = sp(f\"mkdir -p {out_dir}\", capture_output = capture_output)\n",
    "        \n",
    "    print(\"All done!\")\n",
    "    # Moving back to original directory\n",
    "    os.chdir(old_cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a335e33a-caf0-4571-80df-e43eda9a0ab4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting control_script_debug.ipynb\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # in_notebook() is checked in the export function\n",
    "    export_to_py(\"control_script_debug\", pj(_MODULE_DIR, \"control_script\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
